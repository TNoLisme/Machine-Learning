{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MALLORN TDE Classification v1.2 - Balanced Feature Engineering\n",
    "\n",
    "**Chiến lược:** Giữ TẤT CẢ features của v1 + THÊM features theo hướng dẫn từ paper\n",
    "\n",
    "v1.1 thất bại (0.3989) vì chúng ta giảm features quá mạnh.\n",
    "v1.2 giữ mọi thứ từ v1 và thêm insights mới."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path('../mallorn-astronomical-classification-challenge')\n",
    "np.random.seed(42)\n",
    "\n",
    "BANDS = ['u', 'g', 'r', 'i', 'z', 'y']\n",
    "N_SPLITS = 20\n",
    "SNR_THRESHOLD = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tải Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train objects: 3043\n",
      "Test objects: 7135\n",
      "TDE ratio in train: 0.0486\n"
     ]
    }
   ],
   "source": [
    "train_log = pd.read_csv(DATA_DIR / 'train_log.csv')\n",
    "test_log = pd.read_csv(DATA_DIR / 'test_log.csv')\n",
    "\n",
    "print(f\"Train objects: {len(train_log)}\")\n",
    "print(f\"Test objects: {len(test_log)}\")\n",
    "print(f\"TDE ratio in train: {train_log['target'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các Hàm Features V1 Gốc (GIỮ TẤT CẢ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_band_statistics(flux, flux_err, time):\n",
    "    \"\"\"Extract comprehensive statistics for a single band - FULL v1 version.\"\"\"\n",
    "    n = len(flux)\n",
    "    if n == 0:\n",
    "        return {}\n",
    "    \n",
    "    snr = flux / (flux_err + 1e-10)\n",
    "    detections = snr > 3\n",
    "    n_det = np.sum(detections)\n",
    "    \n",
    "    feats = {\n",
    "        'n_obs': n,\n",
    "        'n_det': n_det,\n",
    "        'det_frac': n_det / n if n > 0 else 0,\n",
    "        \n",
    "        'flux_mean': np.mean(flux),\n",
    "        'flux_std': np.std(flux),\n",
    "        'flux_median': np.median(flux),\n",
    "        'flux_max': np.max(flux),\n",
    "        'flux_min': np.min(flux),\n",
    "        'flux_range': np.max(flux) - np.min(flux),\n",
    "        'flux_iqr': np.percentile(flux, 75) - np.percentile(flux, 25),\n",
    "        'flux_skew': stats.skew(flux) if n > 2 else 0,\n",
    "        'flux_kurtosis': stats.kurtosis(flux) if n > 3 else 0,\n",
    "        \n",
    "        'flux_p10': np.percentile(flux, 10),\n",
    "        'flux_p25': np.percentile(flux, 25),\n",
    "        'flux_p75': np.percentile(flux, 75),\n",
    "        'flux_p90': np.percentile(flux, 90),\n",
    "        \n",
    "        'snr_mean': np.mean(snr),\n",
    "        'snr_max': np.max(snr),\n",
    "        'snr_median': np.median(snr),\n",
    "        'snr_std': np.std(snr),\n",
    "        \n",
    "        'err_mean': np.mean(flux_err),\n",
    "        'err_std': np.std(flux_err),\n",
    "    }\n",
    "    \n",
    "    if n > 1:\n",
    "        feats['time_span'] = time[-1] - time[0]\n",
    "        feats['cadence_mean'] = np.mean(np.diff(time))\n",
    "        feats['cadence_std'] = np.std(np.diff(time))\n",
    "    else:\n",
    "        feats['time_span'] = 0\n",
    "        feats['cadence_mean'] = 0\n",
    "        feats['cadence_std'] = 0\n",
    "    \n",
    "    if n_det > 0:\n",
    "        det_flux = flux[detections]\n",
    "        det_time = time[detections]\n",
    "        \n",
    "        feats['det_flux_mean'] = np.mean(det_flux)\n",
    "        feats['det_flux_max'] = np.max(det_flux)\n",
    "        feats['det_duration'] = det_time[-1] - det_time[0] if len(det_time) > 1 else 0\n",
    "        \n",
    "        peak_idx = np.argmax(det_flux)\n",
    "        feats['peak_flux'] = det_flux[peak_idx]\n",
    "        feats['peak_time_rel'] = (det_time[peak_idx] - det_time[0]) / (feats['det_duration'] + 1) if feats['det_duration'] > 0 else 0.5\n",
    "        feats['_peak_time'] = det_time[peak_idx]  # For phase features\n",
    "        \n",
    "        if peak_idx > 0:\n",
    "            rise_dt = det_time[peak_idx] - det_time[0]\n",
    "            rise_df = det_flux[peak_idx] - det_flux[0]\n",
    "            feats['rise_time'] = rise_dt\n",
    "            feats['rise_rate'] = rise_df / (rise_dt + 1e-10)\n",
    "        else:\n",
    "            feats['rise_time'] = 0\n",
    "            feats['rise_rate'] = 0\n",
    "        \n",
    "        if peak_idx < len(det_flux) - 1:\n",
    "            decay_dt = det_time[-1] - det_time[peak_idx]\n",
    "            decay_df = det_flux[peak_idx] - det_flux[-1]\n",
    "            feats['decay_time'] = decay_dt\n",
    "            feats['decay_rate'] = decay_df / (decay_dt + 1e-10)\n",
    "        else:\n",
    "            feats['decay_time'] = 0\n",
    "            feats['decay_rate'] = 0\n",
    "        \n",
    "        if len(det_flux) > 1:\n",
    "            feats['variability'] = np.std(det_flux) / (np.mean(det_flux) + 1e-10)\n",
    "            feats['rms'] = np.sqrt(np.mean(det_flux**2))\n",
    "        else:\n",
    "            feats['variability'] = 0\n",
    "            feats['rms'] = det_flux[0] if len(det_flux) > 0 else 0\n",
    "    else:\n",
    "        for key in ['det_flux_mean', 'det_flux_max', 'det_duration', 'peak_flux', \n",
    "                    'peak_time_rel', '_peak_time', 'rise_time', 'rise_rate', 'decay_time', \n",
    "                    'decay_rate', 'variability', 'rms']:\n",
    "            feats[key] = 0\n",
    "    \n",
    "    above_mean = flux > np.mean(flux)\n",
    "    feats['frac_above_mean'] = np.sum(above_mean) / n\n",
    "    \n",
    "    if n >= 3:\n",
    "        try:\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(time, flux)\n",
    "            feats['trend_slope'] = slope\n",
    "            feats['trend_r2'] = r_value**2\n",
    "        except:\n",
    "            feats['trend_slope'] = 0\n",
    "            feats['trend_r2'] = 0\n",
    "    else:\n",
    "        feats['trend_slope'] = 0\n",
    "        feats['trend_r2'] = 0\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_features(band_data):\n",
    "    \"\"\"Extract color features - FULL v1 version.\"\"\"\n",
    "    colors = {}\n",
    "    \n",
    "    band_fluxes = {}\n",
    "    band_peak_fluxes = {}\n",
    "    \n",
    "    for band in BANDS:\n",
    "        if band in band_data and len(band_data[band]['flux']) > 0:\n",
    "            flux = band_data[band]['flux']\n",
    "            snr = flux / (band_data[band]['flux_err'] + 1e-10)\n",
    "            det_mask = snr > 3\n",
    "            \n",
    "            if np.sum(det_mask) > 0:\n",
    "                det_flux = flux[det_mask]\n",
    "                band_fluxes[band] = np.mean(det_flux)\n",
    "                band_peak_fluxes[band] = np.max(det_flux)\n",
    "            else:\n",
    "                band_fluxes[band] = np.mean(flux) if len(flux) > 0 else 0\n",
    "                band_peak_fluxes[band] = np.max(flux) if len(flux) > 0 else 0\n",
    "        else:\n",
    "            band_fluxes[band] = 0\n",
    "            band_peak_fluxes[band] = 0\n",
    "    \n",
    "    color_pairs = [('u', 'g'), ('u', 'r'), ('u', 'i'), ('g', 'r'), ('g', 'i'), ('r', 'i'), ('i', 'z'), ('z', 'y')]\n",
    "    for b1, b2 in color_pairs:\n",
    "        if band_fluxes[b1] > 0 and band_fluxes[b2] > 0:\n",
    "            colors[f'color_{b1}_{b2}'] = -2.5 * np.log10(band_fluxes[b1] / band_fluxes[b2])\n",
    "            colors[f'color_peak_{b1}_{b2}'] = -2.5 * np.log10(\n",
    "                (band_peak_fluxes[b1] + 1e-10) / (band_peak_fluxes[b2] + 1e-10)\n",
    "            )\n",
    "        else:\n",
    "            colors[f'color_{b1}_{b2}'] = 0\n",
    "            colors[f'color_peak_{b1}_{b2}'] = 0\n",
    "    \n",
    "    blue_bands = ['u', 'g']\n",
    "    red_bands = ['r', 'i', 'z', 'y']\n",
    "    \n",
    "    blue_flux = sum([band_fluxes[b] for b in blue_bands])\n",
    "    red_flux = sum([band_fluxes[b] for b in red_bands])\n",
    "    total_flux = blue_flux + red_flux\n",
    "    \n",
    "    colors['blue_fraction'] = blue_flux / (total_flux + 1e-10)\n",
    "    colors['u_fraction'] = band_fluxes['u'] / (total_flux + 1e-10)\n",
    "    colors['g_fraction'] = band_fluxes['g'] / (total_flux + 1e-10)\n",
    "    colors['blue_red_ratio'] = blue_flux / (red_flux + 1e-10)\n",
    "    \n",
    "    colors['u_dominance'] = band_fluxes['u'] / (np.max(list(band_fluxes.values())) + 1e-10)\n",
    "    colors['peak_band_is_u'] = 1 if band_peak_fluxes['u'] == max(band_peak_fluxes.values()) else 0\n",
    "    colors['peak_band_is_g'] = 1 if band_peak_fluxes['g'] == max(band_peak_fluxes.values()) else 0\n",
    "    colors['peak_band_is_blue'] = 1 if max(band_peak_fluxes['u'], band_peak_fluxes['g']) >= max(\n",
    "        band_peak_fluxes['r'], band_peak_fluxes['i'], band_peak_fluxes['z'], band_peak_fluxes['y']\n",
    "    ) else 0\n",
    "    \n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_features(band_data):\n",
    "    \"\"\"Extract cross-band temporal features - FULL v1 version.\"\"\"\n",
    "    feats = {}\n",
    "    \n",
    "    all_times = []\n",
    "    all_fluxes = []\n",
    "    all_det_times = []\n",
    "    peak_times = {}\n",
    "    global_peak_time = None\n",
    "    global_peak_flux = 0\n",
    "    \n",
    "    for band in BANDS:\n",
    "        if band in band_data and len(band_data[band]['flux']) > 0:\n",
    "            flux = band_data[band]['flux']\n",
    "            time = band_data[band]['time']\n",
    "            flux_err = band_data[band]['flux_err']\n",
    "            \n",
    "            all_times.extend(time)\n",
    "            all_fluxes.extend(flux)\n",
    "            \n",
    "            snr = flux / (flux_err + 1e-10)\n",
    "            det_mask = snr > 3\n",
    "            if np.sum(det_mask) > 0:\n",
    "                det_time = time[det_mask]\n",
    "                det_flux = flux[det_mask]\n",
    "                all_det_times.extend(det_time)\n",
    "                peak_idx = np.argmax(det_flux)\n",
    "                peak_times[band] = det_time[peak_idx]\n",
    "                if det_flux[peak_idx] > global_peak_flux:\n",
    "                    global_peak_flux = det_flux[peak_idx]\n",
    "                    global_peak_time = det_time[peak_idx]\n",
    "    \n",
    "    if len(all_times) > 0:\n",
    "        feats['total_time_span'] = max(all_times) - min(all_times)\n",
    "        feats['total_observations'] = len(all_times)\n",
    "    else:\n",
    "        feats['total_time_span'] = 0\n",
    "        feats['total_observations'] = 0\n",
    "    \n",
    "    if len(all_det_times) > 0:\n",
    "        feats['detection_time_span'] = max(all_det_times) - min(all_det_times)\n",
    "        feats['total_detections'] = len(all_det_times)\n",
    "    else:\n",
    "        feats['detection_time_span'] = 0\n",
    "        feats['total_detections'] = 0\n",
    "    \n",
    "    if len(peak_times) >= 2:\n",
    "        peak_time_values = list(peak_times.values())\n",
    "        feats['peak_time_spread'] = max(peak_time_values) - min(peak_time_values)\n",
    "        \n",
    "        if 'u' in peak_times and 'r' in peak_times:\n",
    "            feats['peak_delay_u_r'] = peak_times['u'] - peak_times['r']\n",
    "        else:\n",
    "            feats['peak_delay_u_r'] = 0\n",
    "        \n",
    "        if 'g' in peak_times and 'r' in peak_times:\n",
    "            feats['peak_delay_g_r'] = peak_times['g'] - peak_times['r']\n",
    "        else:\n",
    "            feats['peak_delay_g_r'] = 0\n",
    "    else:\n",
    "        feats['peak_time_spread'] = 0\n",
    "        feats['peak_delay_u_r'] = 0\n",
    "        feats['peak_delay_g_r'] = 0\n",
    "    \n",
    "    n_bands_detected = sum([1 for b in BANDS if b in band_data and \n",
    "                           len(band_data[b]['flux']) > 0 and \n",
    "                           np.sum(band_data[b]['flux'] / (band_data[b]['flux_err'] + 1e-10) > 3) > 0])\n",
    "    feats['n_bands_detected'] = n_bands_detected\n",
    "    feats['_global_peak_time'] = global_peak_time\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_variability_features(band_data):\n",
    "    \"\"\"Extract variability features - FULL v1 version.\"\"\"\n",
    "    feats = {}\n",
    "    \n",
    "    all_variabilities = []\n",
    "    \n",
    "    for band in BANDS:\n",
    "        if band in band_data and len(band_data[band]['flux']) > 2:\n",
    "            flux = band_data[band]['flux']\n",
    "            time = band_data[band]['time']\n",
    "            \n",
    "            diffs = np.diff(flux)\n",
    "            feats[f'{band}_flux_diff_std'] = np.std(diffs)\n",
    "            feats[f'{band}_flux_diff_mean'] = np.mean(np.abs(diffs))\n",
    "            \n",
    "            if len(flux) > 3:\n",
    "                sorted_idx = np.argsort(time)\n",
    "                sorted_flux = flux[sorted_idx]\n",
    "                \n",
    "                try:\n",
    "                    coeffs = np.polyfit(range(len(sorted_flux)), sorted_flux, 2)\n",
    "                    poly_fit = np.polyval(coeffs, range(len(sorted_flux)))\n",
    "                    residuals = sorted_flux - poly_fit\n",
    "                    feats[f'{band}_residual_std'] = np.std(residuals)\n",
    "                except:\n",
    "                    feats[f'{band}_residual_std'] = 0\n",
    "            else:\n",
    "                feats[f'{band}_residual_std'] = 0\n",
    "            \n",
    "            cv = np.std(flux) / (np.mean(np.abs(flux)) + 1e-10)\n",
    "            all_variabilities.append(cv)\n",
    "            feats[f'{band}_cv'] = cv\n",
    "        else:\n",
    "            feats[f'{band}_flux_diff_std'] = 0\n",
    "            feats[f'{band}_flux_diff_mean'] = 0\n",
    "            feats[f'{band}_residual_std'] = 0\n",
    "            feats[f'{band}_cv'] = 0\n",
    "    \n",
    "    if len(all_variabilities) > 0:\n",
    "        feats['mean_variability'] = np.mean(all_variabilities)\n",
    "        feats['max_variability'] = np.max(all_variabilities)\n",
    "    else:\n",
    "        feats['mean_variability'] = 0\n",
    "        feats['max_variability'] = 0\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Mới theo Hướng dẫn Paper (THÊM LÊN TRÊN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper_features(band_data, global_peak_time, redshift=0):\n",
    "    \"\"\"NEW paper-guided features - added on top of v1.\"\"\"\n",
    "    feats = {}\n",
    "    \n",
    "    # 1. Detection phase features (some_color metrics)\n",
    "    if global_peak_time is not None and global_peak_time > 0:\n",
    "        n_det_pre = 0\n",
    "        n_det_near = 0\n",
    "        n_det_post = 0\n",
    "        bands_near_peak = set()\n",
    "        has_u_near = False\n",
    "        \n",
    "        for band in BANDS:\n",
    "            if band not in band_data:\n",
    "                continue\n",
    "            flux = band_data[band]['flux']\n",
    "            flux_err = band_data[band]['flux_err']\n",
    "            time = band_data[band]['time']\n",
    "            snr = flux / (flux_err + 1e-10)\n",
    "            det_mask = snr > SNR_THRESHOLD\n",
    "            if np.sum(det_mask) == 0:\n",
    "                continue\n",
    "            det_time = time[det_mask]\n",
    "            for t in det_time:\n",
    "                rel_t = t - global_peak_time\n",
    "                if rel_t < -10:\n",
    "                    n_det_pre += 1\n",
    "                elif -10 <= rel_t <= 10:\n",
    "                    n_det_near += 1\n",
    "                    bands_near_peak.add(band)\n",
    "                    if band == 'u':\n",
    "                        has_u_near = True\n",
    "                elif 10 < rel_t <= 30:\n",
    "                    n_det_post += 1\n",
    "        \n",
    "        feats['n_det_pre_peak'] = n_det_pre\n",
    "        feats['n_det_near_peak'] = n_det_near\n",
    "        feats['n_det_post_peak'] = n_det_post\n",
    "        feats['n_bands_near_peak'] = len(bands_near_peak)\n",
    "        feats['has_u_near_peak'] = 1 if has_u_near else 0\n",
    "        \n",
    "        # some_color score\n",
    "        feats['some_color_score'] = (\n",
    "            (1 if n_det_pre >= 1 else 0) +\n",
    "            (1 if len(bands_near_peak) >= 3 else 0) +\n",
    "            (1 if n_det_post >= 2 else 0)\n",
    "        ) / 3.0\n",
    "    else:\n",
    "        feats['n_det_pre_peak'] = 0\n",
    "        feats['n_det_near_peak'] = 0\n",
    "        feats['n_det_post_peak'] = 0\n",
    "        feats['n_bands_near_peak'] = 0\n",
    "        feats['has_u_near_peak'] = 0\n",
    "        feats['some_color_score'] = 0\n",
    "    \n",
    "    # 2. Redshift-corrected features\n",
    "    z_factor = 1 + redshift if redshift > 0 else 1\n",
    "    \n",
    "    # Get u-r color for z-correction\n",
    "    u_flux = 0\n",
    "    r_flux = 0\n",
    "    if 'u' in band_data and len(band_data['u']['flux']) > 0:\n",
    "        u_flux = np.mean(band_data['u']['flux'])\n",
    "    if 'r' in band_data and len(band_data['r']['flux']) > 0:\n",
    "        r_flux = np.mean(band_data['r']['flux'])\n",
    "    \n",
    "    if u_flux > 0 and r_flux > 0:\n",
    "        color_u_r = -2.5 * np.log10(u_flux / r_flux)\n",
    "        feats['color_u_r_z_norm'] = color_u_r / z_factor\n",
    "    else:\n",
    "        feats['color_u_r_z_norm'] = 0\n",
    "    \n",
    "    # 3. Duration classification\n",
    "    det_span = 0\n",
    "    for band in BANDS:\n",
    "        if band in band_data:\n",
    "            flux = band_data[band]['flux']\n",
    "            flux_err = band_data[band]['flux_err']\n",
    "            time = band_data[band]['time']\n",
    "            snr = flux / (flux_err + 1e-10)\n",
    "            det_mask = snr > 3\n",
    "            if np.sum(det_mask) > 1:\n",
    "                det_time = time[det_mask]\n",
    "                det_span = max(det_span, det_time[-1] - det_time[0])\n",
    "    \n",
    "    # TDEs last ~400 days, SNe ~100-150 days\n",
    "    if det_span > 300:\n",
    "        feats['duration_class'] = 2  # Long (TDE-like)\n",
    "    elif det_span > 150:\n",
    "        feats['duration_class'] = 1  # Medium\n",
    "    else:\n",
    "        feats['duration_class'] = 0  # Short (SN-like)\n",
    "    \n",
    "    # 4. Smoothness features (AGN vs TDE)\n",
    "    for band in ['g', 'r']:\n",
    "        if band in band_data and len(band_data[band]['flux']) > 4:\n",
    "            flux = band_data[band]['flux']\n",
    "            time = band_data[band]['time']\n",
    "            sorted_idx = np.argsort(time)\n",
    "            flux_sorted = flux[sorted_idx]\n",
    "            \n",
    "            # Autocorrelation\n",
    "            try:\n",
    "                autocorr = np.corrcoef(flux_sorted[:-1], flux_sorted[1:])[0, 1]\n",
    "                feats[f'{band}_autocorr'] = autocorr if not np.isnan(autocorr) else 0\n",
    "            except:\n",
    "                feats[f'{band}_autocorr'] = 0\n",
    "            \n",
    "            # Sign changes\n",
    "            if len(flux_sorted) > 2:\n",
    "                diffs = np.diff(flux_sorted)\n",
    "                sign_changes = np.sum(np.diff(np.sign(diffs)) != 0)\n",
    "                feats[f'{band}_sign_change_rate'] = sign_changes / (len(diffs) - 1 + 1e-10)\n",
    "            else:\n",
    "                feats[f'{band}_sign_change_rate'] = 0\n",
    "        else:\n",
    "            feats[f'{band}_autocorr'] = 0\n",
    "            feats[f'{band}_sign_change_rate'] = 0\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trích Xuất Features Chủ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_object(obj_id, lc_df, redshift=0):\n",
    "    \"\"\"Extract ALL features: v1 original + paper additions.\"\"\"\n",
    "    features = {'object_id': obj_id}\n",
    "    \n",
    "    obj_data = lc_df[lc_df['object_id'] == obj_id]\n",
    "    if len(obj_data) == 0:\n",
    "        return None\n",
    "    \n",
    "    band_data = {}\n",
    "    for band in BANDS:\n",
    "        band_df = obj_data[obj_data['Filter'] == band].sort_values('Time (MJD)')\n",
    "        if len(band_df) > 0:\n",
    "            band_data[band] = {\n",
    "                'flux': band_df['Flux'].values,\n",
    "                'flux_err': band_df['Flux_err'].values,\n",
    "                'time': band_df['Time (MJD)'].values\n",
    "            }\n",
    "    \n",
    "    # V1 ORIGINAL FEATURES (ALL)\n",
    "    for band in BANDS:\n",
    "        if band in band_data:\n",
    "            band_feats = extract_band_statistics(\n",
    "                band_data[band]['flux'],\n",
    "                band_data[band]['flux_err'],\n",
    "                band_data[band]['time']\n",
    "            )\n",
    "            for key, value in band_feats.items():\n",
    "                if not key.startswith('_'):  # Skip internal keys\n",
    "                    features[f'{band}_{key}'] = value\n",
    "        else:\n",
    "            for key in ['n_obs', 'n_det', 'det_frac', 'flux_mean', 'flux_std', 'flux_median',\n",
    "                       'flux_max', 'flux_min', 'flux_range', 'flux_iqr', 'flux_skew', 'flux_kurtosis',\n",
    "                       'flux_p10', 'flux_p25', 'flux_p75', 'flux_p90', 'snr_mean', 'snr_max',\n",
    "                       'snr_median', 'snr_std', 'err_mean', 'err_std', 'time_span', 'cadence_mean',\n",
    "                       'cadence_std', 'det_flux_mean', 'det_flux_max', 'det_duration', 'peak_flux',\n",
    "                       'peak_time_rel', 'rise_time', 'rise_rate', 'decay_time', 'decay_rate',\n",
    "                       'variability', 'rms', 'frac_above_mean', 'trend_slope', 'trend_r2']:\n",
    "                features[f'{band}_{key}'] = 0\n",
    "    \n",
    "    # V1 color features\n",
    "    color_feats = extract_color_features(band_data)\n",
    "    features.update(color_feats)\n",
    "    \n",
    "    # V1 temporal features\n",
    "    temporal_feats = extract_temporal_features(band_data)\n",
    "    global_peak_time = temporal_feats.pop('_global_peak_time', None)\n",
    "    features.update(temporal_feats)\n",
    "    \n",
    "    # V1 variability features\n",
    "    var_feats = extract_variability_features(band_data)\n",
    "    features.update(var_feats)\n",
    "    \n",
    "    # NEW: Paper-guided features (ADDITIONS)\n",
    "    paper_feats = extract_paper_features(band_data, global_peak_time, redshift)\n",
    "    features.update(paper_feats)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xử lý Dữ liệu Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Processing TRAINING data from ALL 20 splits...\n",
      "============================================================\n",
      "  Processing split_01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 155 objects, total: 155\n",
      "  Processing split_02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 170 objects, total: 325\n",
      "  Processing split_03...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 138 objects, total: 463\n",
      "  Processing split_04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 145 objects, total: 608\n",
      "  Processing split_05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 165 objects, total: 773\n",
      "  Processing split_06...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 155 objects, total: 928\n",
      "  Processing split_07...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 165 objects, total: 1093\n",
      "  Processing split_08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 162 objects, total: 1255\n",
      "  Processing split_09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 128 objects, total: 1383\n",
      "  Processing split_10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 144 objects, total: 1527\n",
      "  Processing split_11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 146 objects, total: 1673\n",
      "  Processing split_12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 155 objects, total: 1828\n",
      "  Processing split_13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 143 objects, total: 1971\n",
      "  Processing split_14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 154 objects, total: 2125\n",
      "  Processing split_15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 158 objects, total: 2283\n",
      "  Processing split_16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 155 objects, total: 2438\n",
      "  Processing split_17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 153 objects, total: 2591\n",
      "  Processing split_18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 152 objects, total: 2743\n",
      "  Processing split_19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 147 objects, total: 2890\n",
      "  Processing split_20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 153 objects, total: 3043\n",
      "\n",
      "Total training features: 3043\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Processing TRAINING data from ALL 20 splits...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_z_lookup = dict(zip(train_log['object_id'], train_log['Z']))\n",
    "train_features_list = []\n",
    "train_object_ids = set(train_log['object_id'].values)\n",
    "\n",
    "for split_num in range(1, N_SPLITS + 1):\n",
    "    split_name = f'split_{split_num:02d}'\n",
    "    lc_file = DATA_DIR / split_name / 'train_full_lightcurves.csv'\n",
    "    \n",
    "    if not lc_file.exists():\n",
    "        print(f\"  {split_name}: train file not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Processing {split_name}...\")\n",
    "    lc_df = pd.read_csv(lc_file)\n",
    "    \n",
    "    object_ids = lc_df['object_id'].unique()\n",
    "    object_ids = [oid for oid in object_ids if oid in train_object_ids]\n",
    "    \n",
    "    for obj_id in tqdm(object_ids, desc=f\"  {split_name}\", leave=False):\n",
    "        z = train_z_lookup.get(obj_id, 0)\n",
    "        feats = extract_features_for_object(obj_id, lc_df, redshift=z)\n",
    "        if feats is not None:\n",
    "            train_features_list.append(feats)\n",
    "    \n",
    "    print(f\"    Processed {len(object_ids)} objects, total: {len(train_features_list)}\")\n",
    "\n",
    "train_features = pd.DataFrame(train_features_list)\n",
    "print(f\"\\nTotal training features: {len(train_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xử lý Dữ liệu Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing TEST data from ALL 20 splits...\n",
      "============================================================\n",
      "  Processing split_01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 364 objects, total: 364\n",
      "  Processing split_02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 414 objects, total: 778\n",
      "  Processing split_03...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 338 objects, total: 1116\n",
      "  Processing split_04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 332 objects, total: 1448\n",
      "  Processing split_05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 375 objects, total: 1823\n",
      "  Processing split_06...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 374 objects, total: 2197\n",
      "  Processing split_07...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 398 objects, total: 2595\n",
      "  Processing split_08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 387 objects, total: 2982\n",
      "  Processing split_09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 289 objects, total: 3271\n",
      "  Processing split_10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 331 objects, total: 3602\n",
      "  Processing split_11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 325 objects, total: 3927\n",
      "  Processing split_12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 353 objects, total: 4280\n",
      "  Processing split_13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 379 objects, total: 4659\n",
      "  Processing split_14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 351 objects, total: 5010\n",
      "  Processing split_15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 342 objects, total: 5352\n",
      "  Processing split_16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 354 objects, total: 5706\n",
      "  Processing split_17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 351 objects, total: 6057\n",
      "  Processing split_18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 345 objects, total: 6402\n",
      "  Processing split_19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 375 objects, total: 6777\n",
      "  Processing split_20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 358 objects, total: 7135\n",
      "\n",
      "Total test features: 7135\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Processing TEST data from ALL 20 splits...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_z_lookup = dict(zip(test_log['object_id'], test_log['Z']))\n",
    "test_features_list = []\n",
    "test_object_ids = set(test_log['object_id'].values)\n",
    "\n",
    "for split_num in range(1, N_SPLITS + 1):\n",
    "    split_name = f'split_{split_num:02d}'\n",
    "    lc_file = DATA_DIR / split_name / 'test_full_lightcurves.csv'\n",
    "    \n",
    "    if not lc_file.exists():\n",
    "        print(f\"  {split_name}: test file not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Processing {split_name}...\")\n",
    "    lc_df = pd.read_csv(lc_file)\n",
    "    \n",
    "    object_ids = lc_df['object_id'].unique()\n",
    "    object_ids = [oid for oid in object_ids if oid in test_object_ids]\n",
    "    \n",
    "    for obj_id in tqdm(object_ids, desc=f\"  {split_name}\", leave=False):\n",
    "        z = test_z_lookup.get(obj_id, 0)\n",
    "        feats = extract_features_for_object(obj_id, lc_df, redshift=z)\n",
    "        if feats is not None:\n",
    "            test_features_list.append(feats)\n",
    "    \n",
    "    print(f\"    Processed {len(object_ids)} objects, total: {len(test_features_list)}\")\n",
    "\n",
    "test_features = pd.DataFrame(test_features_list)\n",
    "print(f\"\\nTotal test features: {len(test_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gộp & Làm sạch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3043, 308)\n",
      "Test shape: (7135, 308)\n"
     ]
    }
   ],
   "source": [
    "train_features = train_features.merge(\n",
    "    train_log[['object_id', 'Z', 'EBV', 'target']], \n",
    "    on='object_id', how='left'\n",
    ")\n",
    "test_features = test_features.merge(\n",
    "    test_log[['object_id', 'Z', 'Z_err', 'EBV']], \n",
    "    on='object_id', how='left'\n",
    ")\n",
    "\n",
    "train_features = train_features.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "test_features = test_features.fillna(0).replace([np.inf, -np.inf], 0)\n",
    "\n",
    "print(f\"Train shape: {train_features.shape}\")\n",
    "print(f\"Test shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "v1.2 FEATURE ENGINEERING COMPLETE\n",
      "============================================================\n",
      "Features: 306\n",
      "  = v1 original (~280) + paper additions (~15)\n",
      "\n",
      "Next: Run 02_model_training.ipynb\n"
     ]
    }
   ],
   "source": [
    "train_features.to_csv('train_features.csv', index=False)\n",
    "test_features.to_csv('test_features.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"v1.2 FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Features: {len(train_features.columns) - 2}\")\n",
    "print(f\"  = v1 original (~280) + paper additions (~15)\")\n",
    "print(\"\\nNext: Run 02_model_training.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
