{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Phân Loại TDE MALLORN - Trích Xuất Đặc Trưng (Feature Engineering)\n\n**Mục tiêu:** Trích xuất features toàn diện từ lightcurves để phân loại TDE.\n\n**Key insights từ paper và data exploration:**\n- **TDEs có màu xanh (blue)** - mạnh ở u-band → Cần features về color\n- **TDEs kéo dài lâu** (~400+ days vs SNe ~100-150 days) → Cần features về temporal/duration\n- **TDEs có evolution smooth**, ít variability hơn AGN → Cần features về variability\n- **Class imbalance nghiêm trọng** (~6% TDEs) → Cần xử lý trong training\n\n**Các nhóm features được trích xuất:**\n1. **Per-band statistics** (6 bands × ~35 features): flux stats, SNR, detection counts, rise/decay\n2. **Color features** (~20 features): u-g, u-r, blue_fraction, peak_band\n3. **Temporal features** (~10 features): duration, peak timing, cross-band sync\n4. **Variability features** (~15 features): smoothness, residuals, coefficient of variation"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from scipy.interpolate import interp1d\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path('../mallorn-astronomical-classification-challenge')\n",
    "np.random.seed(42)\n",
    "\n",
    "BANDS = ['u', 'g', 'r', 'i', 'z', 'y']\n",
    "N_SPLITS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Load Metadata và Cấu Hình\n\nThiết lập các constants và đọc metadata từ train_log và test_log."
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train objects: 3043\n",
      "Test objects: 7135\n",
      "TDE ratio in train: 0.0486\n",
      "\n",
      "Expected submission rows: 7135\n"
     ]
    }
   ],
   "source": [
    "train_log = pd.read_csv(DATA_DIR / 'train_log.csv')\n",
    "test_log = pd.read_csv(DATA_DIR / 'test_log.csv')\n",
    "\n",
    "print(f\"Train objects: {len(train_log)}\")\n",
    "print(f\"Test objects: {len(test_log)}\")\n",
    "print(f\"TDE ratio in train: {train_log['target'].mean():.4f}\")\n",
    "print(f\"\\nExpected submission rows: {len(test_log)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Định Nghĩa Hàm Trích Xuất Features\n\nCác hàm này sẽ xử lý lightcurve và trích xuất features theo từng nhóm."
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_magnitude(flux, flux_err):\n",
    "    \"\"\"Convert flux to magnitude with error handling.\"\"\"\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        mag = -2.5 * np.log10(np.maximum(flux, 1e-10))\n",
    "        mag_err = 2.5 / np.log(10) * flux_err / np.maximum(np.abs(flux), 1e-10)\n",
    "    return mag, mag_err\n",
    "\n",
    "\n",
    "def extract_band_statistics(flux, flux_err, time):\n",
    "    \"\"\"Extract comprehensive statistics for a single band.\"\"\"\n",
    "    n = len(flux)\n",
    "    if n == 0:\n",
    "        return {}\n",
    "    \n",
    "    snr = flux / (flux_err + 1e-10)\n",
    "    detections = snr > 3\n",
    "    n_det = np.sum(detections)\n",
    "    \n",
    "    feats = {\n",
    "        'n_obs': n,\n",
    "        'n_det': n_det,\n",
    "        'det_frac': n_det / n if n > 0 else 0,\n",
    "        \n",
    "        'flux_mean': np.mean(flux),\n",
    "        'flux_std': np.std(flux),\n",
    "        'flux_median': np.median(flux),\n",
    "        'flux_max': np.max(flux),\n",
    "        'flux_min': np.min(flux),\n",
    "        'flux_range': np.max(flux) - np.min(flux),\n",
    "        'flux_iqr': np.percentile(flux, 75) - np.percentile(flux, 25),\n",
    "        'flux_skew': stats.skew(flux) if n > 2 else 0,\n",
    "        'flux_kurtosis': stats.kurtosis(flux) if n > 3 else 0,\n",
    "        \n",
    "        'flux_p10': np.percentile(flux, 10),\n",
    "        'flux_p25': np.percentile(flux, 25),\n",
    "        'flux_p75': np.percentile(flux, 75),\n",
    "        'flux_p90': np.percentile(flux, 90),\n",
    "        \n",
    "        'snr_mean': np.mean(snr),\n",
    "        'snr_max': np.max(snr),\n",
    "        'snr_median': np.median(snr),\n",
    "        'snr_std': np.std(snr),\n",
    "        \n",
    "        'err_mean': np.mean(flux_err),\n",
    "        'err_std': np.std(flux_err),\n",
    "    }\n",
    "    \n",
    "    if n > 1:\n",
    "        feats['time_span'] = time[-1] - time[0]\n",
    "        feats['cadence_mean'] = np.mean(np.diff(time))\n",
    "        feats['cadence_std'] = np.std(np.diff(time))\n",
    "    else:\n",
    "        feats['time_span'] = 0\n",
    "        feats['cadence_mean'] = 0\n",
    "        feats['cadence_std'] = 0\n",
    "    \n",
    "    if n_det > 0:\n",
    "        det_flux = flux[detections]\n",
    "        det_time = time[detections]\n",
    "        \n",
    "        feats['det_flux_mean'] = np.mean(det_flux)\n",
    "        feats['det_flux_max'] = np.max(det_flux)\n",
    "        feats['det_duration'] = det_time[-1] - det_time[0] if len(det_time) > 1 else 0\n",
    "        \n",
    "        peak_idx = np.argmax(det_flux)\n",
    "        feats['peak_flux'] = det_flux[peak_idx]\n",
    "        feats['peak_time_rel'] = (det_time[peak_idx] - det_time[0]) / (feats['det_duration'] + 1) if feats['det_duration'] > 0 else 0.5\n",
    "        \n",
    "        if peak_idx > 0:\n",
    "            rise_dt = det_time[peak_idx] - det_time[0]\n",
    "            rise_df = det_flux[peak_idx] - det_flux[0]\n",
    "            feats['rise_time'] = rise_dt\n",
    "            feats['rise_rate'] = rise_df / (rise_dt + 1e-10)\n",
    "        else:\n",
    "            feats['rise_time'] = 0\n",
    "            feats['rise_rate'] = 0\n",
    "        \n",
    "        if peak_idx < len(det_flux) - 1:\n",
    "            decay_dt = det_time[-1] - det_time[peak_idx]\n",
    "            decay_df = det_flux[peak_idx] - det_flux[-1]\n",
    "            feats['decay_time'] = decay_dt\n",
    "            feats['decay_rate'] = decay_df / (decay_dt + 1e-10)\n",
    "        else:\n",
    "            feats['decay_time'] = 0\n",
    "            feats['decay_rate'] = 0\n",
    "        \n",
    "        if len(det_flux) > 1:\n",
    "            feats['variability'] = np.std(det_flux) / (np.mean(det_flux) + 1e-10)\n",
    "            feats['rms'] = np.sqrt(np.mean(det_flux**2))\n",
    "        else:\n",
    "            feats['variability'] = 0\n",
    "            feats['rms'] = det_flux[0] if len(det_flux) > 0 else 0\n",
    "    else:\n",
    "        for key in ['det_flux_mean', 'det_flux_max', 'det_duration', 'peak_flux', \n",
    "                    'peak_time_rel', 'rise_time', 'rise_rate', 'decay_time', \n",
    "                    'decay_rate', 'variability', 'rms']:\n",
    "            feats[key] = 0\n",
    "    \n",
    "    above_mean = flux > np.mean(flux)\n",
    "    feats['frac_above_mean'] = np.sum(above_mean) / n\n",
    "    \n",
    "    if n >= 3:\n",
    "        try:\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(time, flux)\n",
    "            feats['trend_slope'] = slope\n",
    "            feats['trend_r2'] = r_value**2\n",
    "        except:\n",
    "            feats['trend_slope'] = 0\n",
    "            feats['trend_r2'] = 0\n",
    "    else:\n",
    "        feats['trend_slope'] = 0\n",
    "        feats['trend_r2'] = 0\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_features(band_data):\n",
    "    \"\"\"Extract color features - CRITICAL for TDE identification.\n",
    "    \n",
    "    Paper insight: TDEs are blue (strong u-band emission)\n",
    "    \"\"\"\n",
    "    colors = {}\n",
    "    \n",
    "    band_fluxes = {}\n",
    "    band_peak_fluxes = {}\n",
    "    \n",
    "    for band in BANDS:\n",
    "        if band in band_data and len(band_data[band]['flux']) > 0:\n",
    "            flux = band_data[band]['flux']\n",
    "            snr = flux / (band_data[band]['flux_err'] + 1e-10)\n",
    "            det_mask = snr > 3\n",
    "            \n",
    "            if np.sum(det_mask) > 0:\n",
    "                det_flux = flux[det_mask]\n",
    "                band_fluxes[band] = np.mean(det_flux)\n",
    "                band_peak_fluxes[band] = np.max(det_flux)\n",
    "            else:\n",
    "                band_fluxes[band] = np.mean(flux) if len(flux) > 0 else 0\n",
    "                band_peak_fluxes[band] = np.max(flux) if len(flux) > 0 else 0\n",
    "        else:\n",
    "            band_fluxes[band] = 0\n",
    "            band_peak_fluxes[band] = 0\n",
    "    \n",
    "    color_pairs = [('u', 'g'), ('u', 'r'), ('u', 'i'), ('g', 'r'), ('g', 'i'), ('r', 'i'), ('i', 'z'), ('z', 'y')]\n",
    "    for b1, b2 in color_pairs:\n",
    "        if band_fluxes[b1] > 0 and band_fluxes[b2] > 0:\n",
    "            colors[f'color_{b1}_{b2}'] = -2.5 * np.log10(band_fluxes[b1] / band_fluxes[b2])\n",
    "            colors[f'color_peak_{b1}_{b2}'] = -2.5 * np.log10(\n",
    "                (band_peak_fluxes[b1] + 1e-10) / (band_peak_fluxes[b2] + 1e-10)\n",
    "            )\n",
    "        else:\n",
    "            colors[f'color_{b1}_{b2}'] = 0\n",
    "            colors[f'color_peak_{b1}_{b2}'] = 0\n",
    "    \n",
    "    blue_bands = ['u', 'g']\n",
    "    red_bands = ['r', 'i', 'z', 'y']\n",
    "    \n",
    "    blue_flux = sum([band_fluxes[b] for b in blue_bands])\n",
    "    red_flux = sum([band_fluxes[b] for b in red_bands])\n",
    "    total_flux = blue_flux + red_flux\n",
    "    \n",
    "    colors['blue_fraction'] = blue_flux / (total_flux + 1e-10)\n",
    "    colors['u_fraction'] = band_fluxes['u'] / (total_flux + 1e-10)\n",
    "    colors['g_fraction'] = band_fluxes['g'] / (total_flux + 1e-10)\n",
    "    colors['blue_red_ratio'] = blue_flux / (red_flux + 1e-10)\n",
    "    \n",
    "    colors['u_dominance'] = band_fluxes['u'] / (np.max(list(band_fluxes.values())) + 1e-10)\n",
    "    colors['peak_band_is_u'] = 1 if band_peak_fluxes['u'] == max(band_peak_fluxes.values()) else 0\n",
    "    colors['peak_band_is_g'] = 1 if band_peak_fluxes['g'] == max(band_peak_fluxes.values()) else 0\n",
    "    colors['peak_band_is_blue'] = 1 if max(band_peak_fluxes['u'], band_peak_fluxes['g']) >= max(\n",
    "        band_peak_fluxes['r'], band_peak_fluxes['i'], band_peak_fluxes['z'], band_peak_fluxes['y']\n",
    "    ) else 0\n",
    "    \n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_features(band_data):\n",
    "    \"\"\"Extract cross-band temporal features.\n",
    "    \n",
    "    Paper insight: TDEs have longer duration (~400 days) vs SNe (~100-150 days)\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    \n",
    "    all_times = []\n",
    "    all_fluxes = []\n",
    "    all_det_times = []\n",
    "    peak_times = {}\n",
    "    \n",
    "    for band in BANDS:\n",
    "        if band in band_data and len(band_data[band]['flux']) > 0:\n",
    "            flux = band_data[band]['flux']\n",
    "            time = band_data[band]['time']\n",
    "            flux_err = band_data[band]['flux_err']\n",
    "            \n",
    "            all_times.extend(time)\n",
    "            all_fluxes.extend(flux)\n",
    "            \n",
    "            snr = flux / (flux_err + 1e-10)\n",
    "            det_mask = snr > 3\n",
    "            if np.sum(det_mask) > 0:\n",
    "                det_time = time[det_mask]\n",
    "                det_flux = flux[det_mask]\n",
    "                all_det_times.extend(det_time)\n",
    "                peak_times[band] = det_time[np.argmax(det_flux)]\n",
    "    \n",
    "    if len(all_times) > 0:\n",
    "        feats['total_time_span'] = max(all_times) - min(all_times)\n",
    "        feats['total_observations'] = len(all_times)\n",
    "    else:\n",
    "        feats['total_time_span'] = 0\n",
    "        feats['total_observations'] = 0\n",
    "    \n",
    "    if len(all_det_times) > 0:\n",
    "        feats['detection_time_span'] = max(all_det_times) - min(all_det_times)\n",
    "        feats['total_detections'] = len(all_det_times)\n",
    "    else:\n",
    "        feats['detection_time_span'] = 0\n",
    "        feats['total_detections'] = 0\n",
    "    \n",
    "    if len(peak_times) >= 2:\n",
    "        peak_time_values = list(peak_times.values())\n",
    "        feats['peak_time_spread'] = max(peak_time_values) - min(peak_time_values)\n",
    "        \n",
    "        if 'u' in peak_times and 'r' in peak_times:\n",
    "            feats['peak_delay_u_r'] = peak_times['u'] - peak_times['r']\n",
    "        else:\n",
    "            feats['peak_delay_u_r'] = 0\n",
    "        \n",
    "        if 'g' in peak_times and 'r' in peak_times:\n",
    "            feats['peak_delay_g_r'] = peak_times['g'] - peak_times['r']\n",
    "        else:\n",
    "            feats['peak_delay_g_r'] = 0\n",
    "    else:\n",
    "        feats['peak_time_spread'] = 0\n",
    "        feats['peak_delay_u_r'] = 0\n",
    "        feats['peak_delay_g_r'] = 0\n",
    "    \n",
    "    n_bands_detected = sum([1 for b in BANDS if b in band_data and \n",
    "                           len(band_data[b]['flux']) > 0 and \n",
    "                           np.sum(band_data[b]['flux'] / (band_data[b]['flux_err'] + 1e-10) > 3) > 0])\n",
    "    feats['n_bands_detected'] = n_bands_detected\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_variability_features(band_data):\n",
    "    \"\"\"Extract variability features to distinguish TDEs from AGN.\n",
    "    \n",
    "    Paper insight: AGN have stochastic variability, TDEs are smoother\n",
    "    \"\"\"\n",
    "    feats = {}\n",
    "    \n",
    "    all_variabilities = []\n",
    "    \n",
    "    for band in BANDS:\n",
    "        if band in band_data and len(band_data[band]['flux']) > 2:\n",
    "            flux = band_data[band]['flux']\n",
    "            time = band_data[band]['time']\n",
    "            \n",
    "            diffs = np.diff(flux)\n",
    "            feats[f'{band}_flux_diff_std'] = np.std(diffs)\n",
    "            feats[f'{band}_flux_diff_mean'] = np.mean(np.abs(diffs))\n",
    "            \n",
    "            if len(flux) > 3:\n",
    "                sorted_idx = np.argsort(time)\n",
    "                sorted_flux = flux[sorted_idx]\n",
    "                \n",
    "                try:\n",
    "                    coeffs = np.polyfit(range(len(sorted_flux)), sorted_flux, 2)\n",
    "                    poly_fit = np.polyval(coeffs, range(len(sorted_flux)))\n",
    "                    residuals = sorted_flux - poly_fit\n",
    "                    feats[f'{band}_residual_std'] = np.std(residuals)\n",
    "                except:\n",
    "                    feats[f'{band}_residual_std'] = 0\n",
    "            else:\n",
    "                feats[f'{band}_residual_std'] = 0\n",
    "            \n",
    "            cv = np.std(flux) / (np.mean(np.abs(flux)) + 1e-10)\n",
    "            all_variabilities.append(cv)\n",
    "            feats[f'{band}_cv'] = cv\n",
    "        else:\n",
    "            feats[f'{band}_flux_diff_std'] = 0\n",
    "            feats[f'{band}_flux_diff_mean'] = 0\n",
    "            feats[f'{band}_residual_std'] = 0\n",
    "            feats[f'{band}_cv'] = 0\n",
    "    \n",
    "    if len(all_variabilities) > 0:\n",
    "        feats['mean_variability'] = np.mean(all_variabilities)\n",
    "        feats['max_variability'] = np.max(all_variabilities)\n",
    "    else:\n",
    "        feats['mean_variability'] = 0\n",
    "        feats['max_variability'] = 0\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_for_object(obj_id, lc_df):\n",
    "    \"\"\"Extract all features for a single object.\"\"\"\n",
    "    features = {'object_id': obj_id}\n",
    "    \n",
    "    obj_data = lc_df[lc_df['object_id'] == obj_id]\n",
    "    if len(obj_data) == 0:\n",
    "        return None\n",
    "    \n",
    "    band_data = {}\n",
    "    for band in BANDS:\n",
    "        band_df = obj_data[obj_data['Filter'] == band].sort_values('Time (MJD)')\n",
    "        if len(band_df) > 0:\n",
    "            band_data[band] = {\n",
    "                'flux': band_df['Flux'].values,\n",
    "                'flux_err': band_df['Flux_err'].values,\n",
    "                'time': band_df['Time (MJD)'].values\n",
    "            }\n",
    "    \n",
    "    for band in BANDS:\n",
    "        if band in band_data:\n",
    "            band_feats = extract_band_statistics(\n",
    "                band_data[band]['flux'],\n",
    "                band_data[band]['flux_err'],\n",
    "                band_data[band]['time']\n",
    "            )\n",
    "            for key, value in band_feats.items():\n",
    "                features[f'{band}_{key}'] = value\n",
    "        else:\n",
    "            for key in ['n_obs', 'n_det', 'det_frac', 'flux_mean', 'flux_std', 'flux_median',\n",
    "                       'flux_max', 'flux_min', 'flux_range', 'flux_iqr', 'flux_skew', 'flux_kurtosis',\n",
    "                       'flux_p10', 'flux_p25', 'flux_p75', 'flux_p90', 'snr_mean', 'snr_max',\n",
    "                       'snr_median', 'snr_std', 'err_mean', 'err_std', 'time_span', 'cadence_mean',\n",
    "                       'cadence_std', 'det_flux_mean', 'det_flux_max', 'det_duration', 'peak_flux',\n",
    "                       'peak_time_rel', 'rise_time', 'rise_rate', 'decay_time', 'decay_rate',\n",
    "                       'variability', 'rms', 'frac_above_mean', 'trend_slope', 'trend_r2']:\n",
    "                features[f'{band}_{key}'] = 0\n",
    "    \n",
    "    color_feats = extract_color_features(band_data)\n",
    "    features.update(color_feats)\n",
    "    \n",
    "    temporal_feats = extract_temporal_features(band_data)\n",
    "    features.update(temporal_feats)\n",
    "    \n",
    "    var_feats = extract_variability_features(band_data)\n",
    "    features.update(var_feats)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Xử Lý TOÀN BỘ Dữ Liệu Training (20 splits)\n\nQuét qua tất cả 20 splits và trích xuất features cho mỗi object trong tập train.\n**Lưu ý:** Quá trình này có thể mất vài phút do cần xử lý 3043 objects."
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Processing TRAINING data from ALL 20 splits...\n",
      "============================================================\n",
      "  Processing split_01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 155 objects, total so far: 155\n",
      "  Processing split_02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 170 objects, total so far: 325\n",
      "  Processing split_03...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 138 objects, total so far: 463\n",
      "  Processing split_04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 145 objects, total so far: 608\n",
      "  Processing split_05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 165 objects, total so far: 773\n",
      "  Processing split_06...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 155 objects, total so far: 928\n",
      "  Processing split_07...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 165 objects, total so far: 1093\n",
      "  Processing split_08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 162 objects, total so far: 1255\n",
      "  Processing split_09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 128 objects, total so far: 1383\n",
      "  Processing split_10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 144 objects, total so far: 1527\n",
      "  Processing split_11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 146 objects, total so far: 1673\n",
      "  Processing split_12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 155 objects, total so far: 1828\n",
      "  Processing split_13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 143 objects, total so far: 1971\n",
      "  Processing split_14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 154 objects, total so far: 2125\n",
      "  Processing split_15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 158 objects, total so far: 2283\n",
      "  Processing split_16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 155 objects, total so far: 2438\n",
      "  Processing split_17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 153 objects, total so far: 2591\n",
      "  Processing split_18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 152 objects, total so far: 2743\n",
      "  Processing split_19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 147 objects, total so far: 2890\n",
      "  Processing split_20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 153 objects, total so far: 3043\n",
      "\n",
      "Total training features extracted: 3043\n",
      "Expected: 3043\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Processing TRAINING data from ALL 20 splits...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_features_list = []\n",
    "train_object_ids = set(train_log['object_id'].values)\n",
    "\n",
    "for split_num in range(1, N_SPLITS + 1):\n",
    "    split_name = f'split_{split_num:02d}'\n",
    "    lc_file = DATA_DIR / split_name / 'train_full_lightcurves.csv'\n",
    "    \n",
    "    if not lc_file.exists():\n",
    "        print(f\"  {split_name}: train file not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Processing {split_name}...\")\n",
    "    lc_df = pd.read_csv(lc_file)\n",
    "    \n",
    "    object_ids = lc_df['object_id'].unique()\n",
    "    object_ids = [oid for oid in object_ids if oid in train_object_ids]\n",
    "    \n",
    "    for obj_id in tqdm(object_ids, desc=f\"  {split_name}\", leave=False):\n",
    "        feats = extract_features_for_object(obj_id, lc_df)\n",
    "        if feats is not None:\n",
    "            train_features_list.append(feats)\n",
    "    \n",
    "    print(f\"    Processed {len(object_ids)} objects, total so far: {len(train_features_list)}\")\n",
    "\n",
    "train_features = pd.DataFrame(train_features_list)\n",
    "print(f\"\\nTotal training features extracted: {len(train_features)}\")\n",
    "print(f\"Expected: {len(train_log)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Xử Lý TOÀN BỘ Dữ Liệu Test (20 splits)\n\nTương tự, xử lý tất cả test objects từ 20 splits.\n**Lưu ý:** 7135 test objects → quá trình này sẽ lâu hơn train."
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing TEST data from ALL 20 splits...\n",
      "============================================================\n",
      "  Processing split_01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 364 objects, total so far: 364\n",
      "  Processing split_02...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 414 objects, total so far: 778\n",
      "  Processing split_03...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 338 objects, total so far: 1116\n",
      "  Processing split_04...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 332 objects, total so far: 1448\n",
      "  Processing split_05...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 375 objects, total so far: 1823\n",
      "  Processing split_06...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 374 objects, total so far: 2197\n",
      "  Processing split_07...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 398 objects, total so far: 2595\n",
      "  Processing split_08...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 387 objects, total so far: 2982\n",
      "  Processing split_09...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 289 objects, total so far: 3271\n",
      "  Processing split_10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 331 objects, total so far: 3602\n",
      "  Processing split_11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 325 objects, total so far: 3927\n",
      "  Processing split_12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 353 objects, total so far: 4280\n",
      "  Processing split_13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 379 objects, total so far: 4659\n",
      "  Processing split_14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 351 objects, total so far: 5010\n",
      "  Processing split_15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 342 objects, total so far: 5352\n",
      "  Processing split_16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 354 objects, total so far: 5706\n",
      "  Processing split_17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 351 objects, total so far: 6057\n",
      "  Processing split_18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 345 objects, total so far: 6402\n",
      "  Processing split_19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 375 objects, total so far: 6777\n",
      "  Processing split_20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed 358 objects, total so far: 7135\n",
      "\n",
      "Total test features extracted: 7135\n",
      "Expected: 7135\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Processing TEST data from ALL 20 splits...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_features_list = []\n",
    "test_object_ids = set(test_log['object_id'].values)\n",
    "\n",
    "for split_num in range(1, N_SPLITS + 1):\n",
    "    split_name = f'split_{split_num:02d}'\n",
    "    lc_file = DATA_DIR / split_name / 'test_full_lightcurves.csv'\n",
    "    \n",
    "    if not lc_file.exists():\n",
    "        print(f\"  {split_name}: test file not found, skipping\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Processing {split_name}...\")\n",
    "    lc_df = pd.read_csv(lc_file)\n",
    "    \n",
    "    object_ids = lc_df['object_id'].unique()\n",
    "    object_ids = [oid for oid in object_ids if oid in test_object_ids]\n",
    "    \n",
    "    for obj_id in tqdm(object_ids, desc=f\"  {split_name}\", leave=False):\n",
    "        feats = extract_features_for_object(obj_id, lc_df)\n",
    "        if feats is not None:\n",
    "            test_features_list.append(feats)\n",
    "    \n",
    "    print(f\"    Processed {len(object_ids)} objects, total so far: {len(test_features_list)}\")\n",
    "\n",
    "test_features = pd.DataFrame(test_features_list)\n",
    "print(f\"\\nTotal test features extracted: {len(test_features)}\")\n",
    "print(f\"Expected: {len(test_log)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Gộp với Metadata (Redshift, EBV)\n\nMerge features với Z (redshift) và EBV (extinction) từ train_log và test_log."
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape after merge: (3043, 296)\n",
      "Test shape after merge: (7135, 297)\n"
     ]
    }
   ],
   "source": [
    "train_features = train_features.merge(\n",
    "    train_log[['object_id', 'Z', 'EBV', 'target']], \n",
    "    on='object_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "test_features = test_features.merge(\n",
    "    test_log[['object_id', 'Z', 'Z_err', 'EBV']], \n",
    "    on='object_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "if 'Z_err' in test_features.columns:\n",
    "    test_features['z_relative_err'] = test_features['Z_err'] / (test_features['Z'] + 1e-10)\n",
    "\n",
    "print(f\"\\nTrain shape after merge: {train_features.shape}\")\n",
    "print(f\"Test shape after merge: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Xử Lý Missing Values và Infinity\n\nFill NaN và inf values để đảm bảo dữ liệu sạch cho model training."
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in train: 5671\n",
      "Missing values in test: 12419\n",
      "After cleaning - Train missing: 0\n",
      "After cleaning - Test missing: 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nMissing values in train: {train_features.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test: {test_features.isnull().sum().sum()}\")\n",
    "\n",
    "train_features = train_features.fillna(0)\n",
    "test_features = test_features.fillna(0)\n",
    "\n",
    "train_features = train_features.replace([np.inf, -np.inf], 0)\n",
    "test_features = test_features.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "print(f\"After cleaning - Train missing: {train_features.isnull().sum().sum()}\")\n",
    "print(f\"After cleaning - Test missing: {test_features.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Kiểm Tra Tính Đầy Đủ Của Dữ Liệu\n\nVerify rằng tất cả objects đều đã được xử lý và không có object nào bị thiếu."
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA VERIFICATION\n",
      "============================================================\n",
      "\n",
      "Train:\n",
      "  Expected objects: 3043\n",
      "  Extracted objects: 3043\n",
      "  Match: ✓\n",
      "\n",
      "Test:\n",
      "  Expected objects: 7135\n",
      "  Extracted objects: 7135\n",
      "  Match: ✓\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTrain:\")\n",
    "print(f\"  Expected objects: {len(train_log)}\")\n",
    "print(f\"  Extracted objects: {len(train_features)}\")\n",
    "print(f\"  Match: {'✓' if len(train_features) == len(train_log) else '✗ MISMATCH!'}\")\n",
    "\n",
    "print(f\"\\nTest:\")\n",
    "print(f\"  Expected objects: {len(test_log)}\")\n",
    "print(f\"  Extracted objects: {len(test_features)}\")\n",
    "print(f\"  Match: {'✓' if len(test_features) == len(test_log) else '✗ MISMATCH!'}\")\n",
    "\n",
    "if len(test_features) != len(test_log):\n",
    "    print(f\"\\n  WARNING: Missing {len(test_log) - len(test_features)} test objects!\")\n",
    "    missing = set(test_log['object_id']) - set(test_features['object_id'])\n",
    "    print(f\"  First few missing: {list(missing)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Lưu Features\n\nLưu train_features.csv và test_features.csv để sử dụng trong bước training."
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Saved files:\n",
      "  - train_features.csv: (3043, 296)\n",
      "  - test_features.csv: (7135, 297)\n",
      "\n",
      "Feature categories:\n",
      "  - Per-band statistics: 6 bands × ~35 features\n",
      "  - Color features: ~20 features\n",
      "  - Temporal features: ~10 features\n",
      "  - Variability features: ~15 features\n",
      "  - Metadata: Z, EBV\n",
      "\n",
      "Total features: 294\n",
      "\n",
      "Next: Run 02_model_training.ipynb\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "train_features.to_csv('train_features.csv', index=False)\n",
    "test_features.to_csv('test_features.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSaved files:\")\n",
    "print(f\"  - train_features.csv: {train_features.shape}\")\n",
    "print(f\"  - test_features.csv: {test_features.shape}\")\n",
    "print(f\"\\nFeature categories:\")\n",
    "print(f\"  - Per-band statistics: 6 bands × ~35 features\")\n",
    "print(f\"  - Color features: ~20 features\")\n",
    "print(f\"  - Temporal features: ~10 features\")\n",
    "print(f\"  - Variability features: ~15 features\")\n",
    "print(f\"  - Metadata: Z, EBV\")\n",
    "print(f\"\\nTotal features: {len(train_features.columns) - 2}\")\n",
    "print(\"\\nNext: Run 02_model_training.ipynb\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (py311)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}